from fastapi import APIRouter, HTTPException, Depends, Query
from typing import List, Optional, Dict, Any
import asyncio
import json
from datetime import datetime
from models.policy import Policy, PolicyRecommendation
from utils.policy_recommender import PolicyRecommender
from crawlers.policy_crawler import PolicyCrawler
from crawlers.youth_policy_crawler import YouthPolicyCrawler
from crawlers.youth_center_crawler import YouthCenterCrawler
from auth.jwt_handler import verify_token
from database.connection import get_db_connection


router = APIRouter(prefix="/policies", tags=["policies"])
recommender = PolicyRecommender()


@router.get("/recommendations")
async def get_policy_recommendations(
    limit: int = Query(10, ge=1, le=50),
    offset: int = Query(0, ge=0),
    token_data: dict = Depends(verify_token)
):
    """ê°œì¸í™”ëœ ì •ì±… ì¶”ì²œ ì¡°íšŒ"""
    try:
        user_id = token_data.get("user_id")
        
        conn = get_db_connection()
        cursor = conn.cursor()
        
        # ì‚¬ìš©ì ë§ì¶¤ ì •ì±… ì¡°íšŒ (ì§€ì—­, ì—°ë ¹ ë“± ê³ ë ¤)
        # ì „ì²´ ê°œìˆ˜ ì¡°íšŒ
        cursor.execute("""
            SELECT COUNT(*) 
            FROM policies
            WHERE is_active = 1 
            AND ([region] = 'ì „êµ­' OR [region] IS NULL OR [region] = '')
        """)
        total_count = cursor.fetchone()[0]
        
        # ì •ì±… ë°ì´í„° ì¡°íšŒ
        cursor.execute(f"""
            SELECT id, source, source_id, title, organization, target, 
                   content, application_period, start_date, end_date,
                   application_url, reference_url, category, [region], details,
                   view_count, created_at
            FROM policies
            WHERE is_active = 1 
            AND ([region] = 'ì „êµ­' OR [region] IS NULL OR [region] = '')
            ORDER BY view_count DESC, created_at DESC
            LIMIT {limit} OFFSET {offset}
        """)
        
        policies = cursor.fetchall()
        conn.close()
        
        result = []
        for p in policies:
            try:
                details = json.loads(p[14]) if p[14] else {}
            except:
                details = {}
            result.append({
                "id": p[0],
                "source": p[1],
                "source_id": p[2], 
                "title": p[3],
                "organization": p[4],
                "target": p[5],
                "content": p[6],
                "application_period": p[7],
                "start_date": p[8],
                "end_date": p[9],
                "application_url": p[10],
                "reference_url": p[11],
                "category": p[12],
                "region": p[13],
                "details": details,
                "view_count": p[15],
                "created_at": p[16],
                "policy": {
                    "id": p[0],
                    "title": p[3],
                    "category": p[12],
                    "description": (p[6][:200] + "...") if p[6] and len(p[6]) > 200 else (p[6] or ""),
                    "url": p[10] or p[11] or ""
                },
                "reason": "ë§ì¶¤ ì¶”ì²œ"
            })
        
        return {
            "data": result,
            "total_count": total_count,
            "page": (offset // limit) + 1,
            "total_pages": (total_count + limit - 1) // limit
        }
    except Exception as e:
        print(f"Error getting policy recommendations: {e}")
        raise HTTPException(status_code=500, detail="ì •ì±… ì¶”ì²œ ì¡°íšŒì— ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤")


@router.get("/popular")
async def get_popular_policies(
    limit: int = Query(10, ge=1, le=50),
    offset: int = Query(0, ge=0),
    region: Optional[str] = Query(None, description="ì§€ì—­ í•„í„°"),
    category: Optional[str] = Query(None, description="ì¹´í…Œê³ ë¦¬ í•„í„°")
):
    """ì¸ê¸° ì •ì±… ì¡°íšŒ (ë¡œê·¸ì¸ ë¶ˆí•„ìš”)"""
    try:
        conn = get_db_connection()
        cursor = conn.cursor()
        
        # ì „ì²´ ê°œìˆ˜ ì¡°íšŒ
        cursor.execute("""
            SELECT COUNT(*) 
            FROM policies
            WHERE is_active = 1
        """)
        total_count = cursor.fetchone()[0]
        
        # ì¿¼ë¦¬ ì‹¤í–‰  
        query = f"""
            SELECT id, source, source_id, title, organization, target, 
                   content, application_period, start_date, end_date,
                   application_url, reference_url, category, region, details,
                   view_count, created_at
            FROM policies
            WHERE is_active = 1
            ORDER BY view_count DESC, created_at DESC LIMIT {limit} OFFSET {offset}
        """
        cursor.execute(query)
        policies = cursor.fetchall()
        conn.close()
        
        result = []
        for p in policies:
            try:
                details = json.loads(p[14]) if p[14] else {}
            except:
                details = {}
            result.append({
                "id": p[0],
                "source": p[1],
                "source_id": p[2],
                "title": p[3],
                "organization": p[4],
                "target": p[5],
                "content": p[6],
                "application_period": p[7],
                "start_date": p[8],
                "end_date": p[9],
                "application_url": p[10],
                "reference_url": p[11],
                "category": p[12],
                "region": p[13],
                "details": details,
                "view_count": p[15],
                "created_at": p[16]
            })
        
        return {
            "data": result,
            "total_count": total_count,
            "page": (offset // limit) + 1,
            "total_pages": (total_count + limit - 1) // limit
        }
    except Exception as e:
        print(f"Error getting popular policies: {e}")
        raise HTTPException(status_code=500, detail="ì¸ê¸° ì •ì±… ì¡°íšŒì— ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤")


@router.post("/view/{policy_id}")
async def record_policy_view(
    policy_id: int,
    token_data: dict = Depends(verify_token)
):
    """ì •ì±… ì¡°íšŒ ê¸°ë¡"""
    try:
        user_id = token_data.get("user_id")
        if not user_id:
            raise HTTPException(status_code=400, detail="ìœ íš¨í•˜ì§€ ì•Šì€ ì‚¬ìš©ì ì •ë³´ì…ë‹ˆë‹¤")
        recommender.record_policy_view(user_id, policy_id)
        return {"message": "ì¡°íšŒ ê¸°ë¡ì´ ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤"}
    except Exception as e:
        print(f"Error recording policy view: {e}")
        raise HTTPException(status_code=500, detail="ì¡°íšŒ ê¸°ë¡ ì €ì¥ì— ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤")


@router.get("/categories")
async def get_policy_categories():
    """ì •ì±… ì¹´í…Œê³ ë¦¬ ëª©ë¡ ì¡°íšŒ"""
    try:
        conn = recommender.get_db_connection()
        cursor = conn.cursor()
        
        cursor.execute("""
            SELECT category, COUNT(*) as count
            FROM policies
            WHERE is_active = 1
            GROUP BY category
            ORDER BY count DESC
        """)
        
        categories = [{"name": row[0], "count": row[1]} for row in cursor.fetchall()]
        conn.close()
        
        return categories
    except Exception as e:
        print(f"Error getting policy categories: {e}")
        raise HTTPException(status_code=500, detail="ì¹´í…Œê³ ë¦¬ ì¡°íšŒì— ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤")


@router.get("/category/{category}", response_model=List[PolicyRecommendation])
async def get_policies_by_category(
    category: str,
    limit: int = Query(20, ge=1, le=50),
    token_data: Optional[dict] = Depends(verify_token)
):
    """ì¹´í…Œê³ ë¦¬ë³„ ì •ì±… ì¡°íšŒ"""
    try:
        conn = recommender.get_db_connection()
        cursor = conn.cursor()
        
        cursor.execute("""
            SELECT id, title, description, content, url, category,
                   target_age_min, target_age_max, target_gender, target_location,
                   tags, view_count, relevance_score, crawled_at
            FROM policies
            WHERE category = ? AND is_active = 1
            ORDER BY view_count DESC, crawled_at DESC
            LIMIT ?
        """, (category, limit))
        
        policies = cursor.fetchall()
        conn.close()
        
        recommendations = []
        for policy_data in policies:
            from datetime import datetime
            import json
            
            policy = Policy(
                id=policy_data[0],
                title=policy_data[1],
                description=policy_data[2],
                content=policy_data[3],
                url=policy_data[4],
                category=policy_data[5],
                target_age_min=policy_data[6],
                target_age_max=policy_data[7],
                target_gender=policy_data[8],
                target_location=policy_data[9],
                tags=json.loads(policy_data[10]) if policy_data[10] else [],
                view_count=policy_data[11],
                relevance_score=policy_data[12],
                crawled_at=datetime.fromisoformat(policy_data[13])
            )
            
            recommendations.append(PolicyRecommendation(
                policy=policy,
                score=policy_data[11],
                reason=f"{category} ì¹´í…Œê³ ë¦¬"
            ))
        
        return recommendations
    except Exception as e:
        print(f"Error getting policies by category: {e}")
        raise HTTPException(status_code=500, detail="ì¹´í…Œê³ ë¦¬ë³„ ì •ì±… ì¡°íšŒì— ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤")


@router.post("/crawl")
async def trigger_crawling():
    """ì •ì±… í¬ë¡¤ë§ ìˆ˜ë™ ì‹¤í–‰ (ê´€ë¦¬ììš©)"""
    try:
        crawler = PolicyCrawler()
        result = await crawler.run_crawling()
        return {"message": "í¬ë¡¤ë§ì´ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤", "result": result}
    except Exception as e:
        print(f"Error triggering crawling: {e}")
        raise HTTPException(status_code=500, detail="í¬ë¡¤ë§ ì‹¤í–‰ì— ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤")


@router.post("/crawl/youth")
async def trigger_youth_policy_crawling():
    """ì²­ë…„ ì •ì±… í¬ë¡¤ë§ ìˆ˜ë™ ì‹¤í–‰ (ê´€ë¦¬ììš©)"""
    try:
        crawler = YouthPolicyCrawler()
        result = await crawler.run_youth_policy_crawling()
        return {"message": "ì²­ë…„ ì •ì±… í¬ë¡¤ë§ì´ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤", "result": result}
    except Exception as e:
        print(f"Error triggering youth policy crawling: {e}")
        raise HTTPException(status_code=500, detail="ì²­ë…„ ì •ì±… í¬ë¡¤ë§ ì‹¤í–‰ì— ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤")


@router.post("/crawl/youth-center")
async def trigger_youth_center_crawling(
    max_pages: int = Query(5, ge=1, le=20, description="í¬ë¡¤ë§í•  í˜ì´ì§€ ìˆ˜")
):
    """ì˜¨í†µì²­ë…„ API í¬ë¡¤ë§ ìˆ˜ë™ ì‹¤í–‰ (ê´€ë¦¬ììš©)"""
    try:
        crawler = YouthCenterCrawler()
        saved, updated = crawler.crawl_all_policies(max_pages=max_pages)
        return {
            "message": "ì˜¨í†µì²­ë…„ ì •ì±… í¬ë¡¤ë§ì´ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤",
            "result": {
                "saved": saved,
                "updated": updated,
                "total": saved + updated
            }
        }
    except Exception as e:
        print(f"Error triggering youth center crawling: {e}")
        raise HTTPException(status_code=500, detail="ì˜¨í†µì²­ë…„ í¬ë¡¤ë§ ì‹¤í–‰ì— ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤")


@router.get("/youth", response_model=List[PolicyRecommendation])
async def get_youth_policies(
    limit: int = Query(10, ge=1, le=50),
    token_data: dict = Depends(verify_token)
):
    """ê°œì¸í™”ëœ ì²­ë…„ ì •ì±… ì¡°íšŒ"""
    try:
        user_id = token_data.get("user_id")
        crawler = YouthPolicyCrawler()
        policies = crawler.get_personalized_policies(user_id, limit)
        
        # PolicyRecommendation í˜•íƒœë¡œ ë³€í™˜
        recommendations = []
        for policy_data in policies:
            from datetime import datetime
            
            policy = Policy(
                id=policy_data['id'],
                title=policy_data['title'],
                description=policy_data['description'],
                content=policy_data['content'],
                url=policy_data['url'],
                category=policy_data['category'],
                target_age_min=policy_data['target_age_min'],
                target_age_max=policy_data['target_age_max'],
                target_gender=None,
                target_location=None,
                tags=policy_data['tags'],
                view_count=0,
                relevance_score=0.0,
                crawled_at=datetime.fromisoformat(policy_data['crawled_at'])
            )
            
            recommendations.append(PolicyRecommendation(
                policy=policy,
                score=100,
                reason="ì²­ë…„ ë§ì¶¤ ì •ì±…"
            ))
        
        return recommendations
        
    except Exception as e:
        print(f"Error getting youth policies: {e}")
        raise HTTPException(status_code=500, detail="ì²­ë…„ ì •ì±… ì¡°íšŒì— ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤")


@router.get("/{policy_id}/ai-summary")  
async def get_policy_ai_summary(
    policy_id: int
):
    """ì •ì±… AI ìš”ì•½ ìƒì„±"""
    try:
        conn = get_db_connection()
        cursor = conn.cursor()
        
        # ì •ì±… ì •ë³´ ì¡°íšŒ
        cursor.execute("""
            SELECT title, content, target, details, application_period, organization
            FROM policies
            WHERE id = ? AND is_active = 1
        """, (policy_id,))
        
        policy = cursor.fetchone()
        conn.close()
        
        if not policy:
            raise HTTPException(status_code=404, detail="ì •ì±…ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤")
        
        title, content, target, details_json, period, org = policy
        
        # ì§€ì—­ ì½”ë“œë¥¼ ì§€ì—­ëª…ìœ¼ë¡œ ë³€í™˜í•˜ëŠ” í•¨ìˆ˜
        def convert_region_codes_to_names(region_codes):
            if not region_codes:
                return "ì „êµ­"
            
            # ì£¼ìš” ì§€ì—­ ì½”ë“œ ë§¤í•‘
            region_map = {
                '11': 'ì„œìš¸íŠ¹ë³„ì‹œ', '26': 'ë¶€ì‚°ê´‘ì—­ì‹œ', '27': 'ëŒ€êµ¬ê´‘ì—­ì‹œ', '28': 'ì¸ì²œê´‘ì—­ì‹œ',
                '29': 'ê´‘ì£¼ê´‘ì—­ì‹œ', '30': 'ëŒ€ì „ê´‘ì—­ì‹œ', '31': 'ìš¸ì‚°ê´‘ì—­ì‹œ', '36': 'ì„¸ì¢…íŠ¹ë³„ìì¹˜ì‹œ',
                '41': 'ê²½ê¸°ë„', '42': 'ê°•ì›ë„', '43': 'ì¶©ì²­ë¶ë„', '44': 'ì¶©ì²­ë‚¨ë„',
                '45': 'ì „ë¼ë¶ë„', '46': 'ì „ë¼ë‚¨ë„', '47': 'ê²½ìƒë¶ë„', '48': 'ê²½ìƒë‚¨ë„',
                '49': 'ì œì£¼íŠ¹ë³„ìì¹˜ë„', '50': 'ì œì£¼íŠ¹ë³„ìì¹˜ë„',
                '50110': 'ì œì£¼ì‹œ', '50130': 'ì„œê·€í¬ì‹œ',
                '44131': 'ì²œì•ˆì‹œ', '44133': 'ê³µì£¼ì‹œ', '44150': 'ë³´ë ¹ì‹œ', '44180': 'ì•„ì‚°ì‹œ',
                '44200': 'ì„œì‚°ì‹œ', '44210': 'ë…¼ì‚°ì‹œ', '44230': 'ê³„ë£¡ì‹œ', '44250': 'ë‹¹ì§„ì‹œ',
                '44270': 'ê¸ˆì‚°êµ°', '44710': 'ì—°ê¸°êµ°', '44760': 'ë³´ì€êµ°', '44770': 'ì˜¥ì²œêµ°',
                '44790': 'ì˜ë™êµ°', '44800': 'ì§„ì²œêµ°', '44810': 'ê´´ì‚°êµ°', '44825': 'ìŒì„±êµ°'
            }
            
            if ',' in region_codes:
                codes = region_codes.split(',')
                regions = []
                for code in codes:
                    code = code.strip()
                    if code in region_map:
                        regions.append(region_map[code])
                    elif code[:2] in region_map:
                        regions.append(region_map[code[:2]])
                return ', '.join(list(set(regions)))
            else:
                code = region_codes.strip()
                if code in region_map:
                    return region_map[code]
                elif code[:2] in region_map:
                    return region_map[code[:2]]
                return "ì „êµ­"

        # target í•„ë“œì—ì„œ ì˜ë¯¸ìˆëŠ” ì •ë³´ë§Œ ì¶”ì¶œ
        def clean_target_info(target_text):
            if not target_text:
                return ""
            
            # ì—°ë ¹ëŒ€ ì •ë³´ë§Œ ì¶”ì¶œ (ë§Œ XXì„¸, ë§Œ XX~XXì„¸)
            import re
            age_patterns = re.findall(r'ë§Œ \d+[~-]?\d*ì„¸?', target_text)
            if age_patterns:
                return ', '.join(age_patterns)
            return ""

        # AI ìš”ì•½ì„ ìœ„í•œ ì •ì±… ì •ë³´ êµ¬ì„±
        cleaned_target = clean_target_info(target)
        
        policy_text = f"""
        ì •ì±…ëª…: {title}
        ì‹œí–‰ê¸°ê´€: {org or ''}
        ì •ì±… ë‚´ìš©: {content or ''}
        """
        
        if cleaned_target:
            policy_text += f"\nì§€ì› ëŒ€ìƒ: {cleaned_target}"
        
        if period:
            policy_text += f"\nì‹ ì²­ ê¸°ê°„: {period}"
        
        # ì„¸ë¶€ ì •ë³´ê°€ ìˆìœ¼ë©´ ì¶”ê°€
        if details_json:
            try:
                details = json.loads(details_json)
                if details.get('explanation'):
                    policy_text += f"\nì •ì±… ì„¤ëª…: {details['explanation']}"
                if details.get('income_condition'):
                    policy_text += f"\nì†Œë“ ì¡°ê±´: {details['income_condition']}"
                if details.get('min_age') and details.get('max_age'):
                    policy_text += f"\nì—°ë ¹ ì¡°ê±´: ë§Œ {details['min_age']}ì„¸ ~ {details['max_age']}ì„¸"
                if details.get('region_code'):
                    region_names = convert_region_codes_to_names(details['region_code'])
                    policy_text += f"\nì ìš© ì§€ì—­: {region_names}"
            except:
                pass
        
        # Gemini AI ìš”ì•½ ìƒì„± (ê°„ë‹¨í•œ ìš”ì•½)
        from ai.policy_chat.gemini_client import GeminiClient
        
        gemini = GeminiClient()
        
        prompt = f"""
        ë‹¤ìŒ ì²­ë…„ ì£¼íƒ ì •ì±…ì„ ë§ˆí¬ë‹¤ìš´ í˜•ì‹ìœ¼ë¡œ ê°„ë‹¨í•˜ê³  ì´í•´í•˜ê¸° ì‰½ê²Œ ìš”ì•½í•´ì£¼ì„¸ìš”:
        
        {policy_text}
        
        ë‹¤ìŒ í˜•ì‹ìœ¼ë¡œ ìš”ì•½í•´ì£¼ì„¸ìš”:
        
        **ëŒ€ìƒ:** ì—°ë ¹ëŒ€ì™€ ì£¼ìš” ì¡°ê±´
        **ì§€ì›ë‚´ìš©:** í•µì‹¬ í˜œíƒê³¼ ê¸ˆì•¡
        **ì‹ ì²­ê¸°ê°„:** ê¸°ê°„ ë˜ëŠ” ìƒì‹œì ‘ìˆ˜ ì—¬ë¶€
        **ì§€ì—­:** í•´ë‹¹ ì§€ì—­ (ì „êµ­ì´ë©´ ìƒëµ)
        
        ê° í•­ëª©ì€ 1ì¤„ë¡œ ê°„ê²°í•˜ê²Œ ì‘ì„±í•˜ê³ , ë¶ˆí•„ìš”í•œ ì„¤ëª…ì€ ì œì™¸í•´ì£¼ì„¸ìš”.
        """
        
        try:
            print(f"ğŸ¤– AI ìš”ì•½ ìƒì„± ì‹œì‘: {title}")
            print(f"ğŸ“ ì…ë ¥ í…ìŠ¤íŠ¸: {policy_text[:200]}...")
            
            # LangChain ìŠ¤íŠ¸ë¦¬ë° ë°©ì‹ìœ¼ë¡œ AI ìš”ì•½ ìƒì„±
            from langchain.schema import HumanMessage
            
            llm = gemini.get_llm()
            messages = [HumanMessage(content=prompt)]
            
            # astreamìœ¼ë¡œ ìŠ¤íŠ¸ë¦¬ë° ì²˜ë¦¬
            summary_parts = []
            async for chunk in llm.astream(messages):
                if hasattr(chunk, 'content'):
                    summary_parts.append(chunk.content)
            
            summary = ''.join(summary_parts)
            
            print(f"âœ… AI ìš”ì•½ ì™„ë£Œ: {summary[:100]}...")
            return {"summary": summary}
        except Exception as e:
            print(f"âŒ AI ìš”ì•½ ìƒì„± ì‹¤íŒ¨: {e}")
            print(f"ğŸ”§ Gemini í´ë¼ì´ì–¸íŠ¸ ìƒíƒœ í™•ì¸ í•„ìš”")
            # ê°œì„ ëœ í´ë°± ìš”ì•½ (ì˜ë¯¸ì—†ëŠ” ì½”ë“œ ì œê±°)
            fallback_summary = f"ì´ ì •ì±…ì€ {org or 'ê´€ë ¨ ê¸°ê´€'}ì—ì„œ ì‹œí–‰í•˜ëŠ” {title} ì •ì±…ì…ë‹ˆë‹¤."
            if cleaned_target:
                fallback_summary += f" {cleaned_target}ë¥¼ ëŒ€ìƒìœ¼ë¡œ í•©ë‹ˆë‹¤."
            return {"summary": fallback_summary}
        
    except HTTPException:
        raise
    except Exception as e:
        print(f"Error getting policy AI summary: {e}")
        raise HTTPException(status_code=500, detail="AI ìš”ì•½ ìƒì„±ì— ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤")


@router.get("/by-title/{title}")
async def get_policy_by_title(title: str):
    """ì •ì±…ëª…ìœ¼ë¡œ ì •ì±… ìƒì„¸ ì •ë³´ ì¡°íšŒ (ì±—ë´‡ìš©)"""
    try:
        print(f"Searching for policy with title: '{title}'")
        conn = get_db_connection()
        cursor = conn.cursor()
        
        # ì •ì±…ëª…ìœ¼ë¡œ ìœ ì—°í•œ ê²€ìƒ‰ (ë¶€ë¶„ ì¼ì¹˜, ê³µë°±/íŠ¹ìˆ˜ë¬¸ì ì œê±°)
        clean_title = title.replace(" ", "").replace("ã€", "").replace("ã€Œ", "")
        search_patterns = [
            title,  # ì •í™•í•œ ë§¤ì¹­
            f"%{title}%",  # ë¶€ë¶„ ë§¤ì¹­
            f"%{clean_title}%",  # ê³µë°±/íŠ¹ìˆ˜ë¬¸ì ì œê±° í›„ ë§¤ì¹­
        ]
        
        # ì—¬ëŸ¬ íŒ¨í„´ìœ¼ë¡œ ê²€ìƒ‰
        cursor.execute("""
            SELECT id, source, source_id, title, organization, target, 
                   content, application_period, start_date, end_date,
                   application_url, reference_url, category, region, details,
                   view_count, created_at
            FROM policies
            WHERE (title = ? OR title LIKE ? OR REPLACE(REPLACE(title, ' ', ''), 'ã€€', '') LIKE ?)
            AND is_active = 1
            ORDER BY 
                CASE 
                    WHEN title = ? THEN 1
                    WHEN title LIKE ? THEN 2
                    ELSE 3
                END,
                view_count DESC
            LIMIT 1
        """, (title, f"%{title}%", f"%{clean_title}%", title, f"%{title}%"))
        
        policy = cursor.fetchone()
        conn.close()
        
        if not policy:
            raise HTTPException(status_code=404, detail="ì •ì±…ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤")
        
        try:
            details = json.loads(policy[14]) if policy[14] else {}
        except:
            details = {}
            
        return {
            "id": policy[0],
            "source": policy[1],
            "source_id": policy[2],
            "title": policy[3],
            "organization": policy[4],
            "target": policy[5],
            "content": policy[6],
            "application_period": policy[7],
            "start_date": policy[8],
            "end_date": policy[9],
            "application_url": policy[10],
            "reference_url": policy[11],
            "category": policy[12],
            "region": policy[13],
            "details": details,
            "view_count": policy[15],
            "created_at": policy[16]
        }
        
    except HTTPException:
        raise
    except Exception as e:
        print(f"Error getting policy by title: {e}")
        raise HTTPException(status_code=500, detail="ì •ì±… ì¡°íšŒì— ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤")


@router.get("/search")
async def search_policies(
    q: str = Query(..., min_length=1),
    limit: int = Query(20, ge=1, le=50),
    token_data: Optional[dict] = Depends(verify_token)
):
    """ì •ì±… ê²€ìƒ‰"""
    try:
        conn = recommender.get_db_connection()
        cursor = conn.cursor()
        
        # ì œëª©, ì„¤ëª…, ë‚´ìš©ì—ì„œ í‚¤ì›Œë“œ ê²€ìƒ‰
        search_query = f"%{q}%"
        cursor.execute("""
            SELECT id, title, description, content, url, category,
                   target_age_min, target_age_max, target_gender, target_location,
                   tags, view_count, relevance_score, crawled_at
            FROM policies
            WHERE (title LIKE ? OR description LIKE ? OR content LIKE ?)
            AND is_active = 1
            ORDER BY view_count DESC, crawled_at DESC
            LIMIT ?
        """, (search_query, search_query, search_query, limit))
        
        policies = cursor.fetchall()
        conn.close()
        
        recommendations = []
        for policy_data in policies:
            from datetime import datetime
            import json
            
            policy = Policy(
                id=policy_data[0],
                title=policy_data[1],
                description=policy_data[2],
                content=policy_data[3],
                url=policy_data[4],
                category=policy_data[5],
                target_age_min=policy_data[6],
                target_age_max=policy_data[7],
                target_gender=policy_data[8],
                target_location=policy_data[9],
                tags=json.loads(policy_data[10]) if policy_data[10] else [],
                view_count=policy_data[11],
                relevance_score=policy_data[12],
                crawled_at=datetime.fromisoformat(policy_data[13])
            )
            
            recommendations.append(PolicyRecommendation(
                policy=policy,
                score=policy_data[11],
                reason=f"'{q}' ê²€ìƒ‰ ê²°ê³¼"
            ))
        
        return recommendations
    except Exception as e:
        print(f"Error searching policies: {e}")
        raise HTTPException(status_code=500, detail="ì •ì±… ê²€ìƒ‰ì— ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤")