"""
ë²¡í„° ì„ë² ë”© ìƒì„± ì‹œìŠ¤í…œ
í•œêµ­ì–´ í…ìŠ¤íŠ¸ë¥¼ ìœ„í•œ ì„ë² ë”© ìƒì„± ë° ê²€ìƒ‰ ê¸°ëŠ¥ ì œê³µ
"""
import os
import json
import pickle
import numpy as np
from pathlib import Path
from typing import List, Dict, Any, Tuple
from datetime import datetime
import logging

try:
    from sentence_transformers import SentenceTransformer
    SENTENCE_TRANSFORMERS_AVAILABLE = True
except ImportError:
    SENTENCE_TRANSFORMERS_AVAILABLE = False
    print("Warning: sentence-transformers not installed. Please install with: pip install sentence-transformers")

try:
    import faiss
    FAISS_AVAILABLE = True
except ImportError:
    FAISS_AVAILABLE = False
    print("Warning: faiss not installed. Please install with: pip install faiss-cpu")

from .document_processor import DocumentProcessor

# ë¡œê±° ì„¤ì •
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)


class VectorEmbedder:
    """ë²¡í„° ì„ë² ë”© ìƒì„± ë° ê´€ë¦¬ í´ë˜ìŠ¤"""
    
    def __init__(
        self, 
        documents_path: str = "ai/rag_documents",
        model_name: str = "all-MiniLM-L6-v2"
    ):
        self.documents_path = Path(documents_path)
        self.processed_path = self.documents_path / "processed"
        self.embeddings_path = self.processed_path / "embeddings"
        self.chunks_path = self.processed_path / "chunks"
        
        # ë””ë ‰í† ë¦¬ ìƒì„±
        self.embeddings_path.mkdir(parents=True, exist_ok=True)
        self.chunks_path.mkdir(parents=True, exist_ok=True)
        
        # ëª¨ë¸ ì„¤ì •
        self.model_name = model_name
        self.model = None
        self.index = None
        self.documents = []
        
        # ë©”íƒ€ë°ì´í„° ê²½ë¡œ
        self.metadata_path = self.processed_path / "metadata.json"
        
        # ë¨¼ì € ê¸°ì¡´ ì„ë² ë”© ë¡œë“œ ì‹œë„
        if not self.load_latest_embeddings():
            # ê¸°ì¡´ ì„ë² ë”©ì´ ì—†ìœ¼ë©´ ëª¨ë¸ ë¡œë“œ
            self._load_model()
        else:
            logger.info("Existing embeddings loaded, model loading skipped for now")
    
    def _load_model(self):
        """ì„ë² ë”© ëª¨ë¸ ë¡œë“œ"""
        if not SENTENCE_TRANSFORMERS_AVAILABLE:
            logger.error("sentence-transformers not available")
            return
            
        try:
            logger.info(f"Loading model: {self.model_name}")
            self.model = SentenceTransformer(self.model_name)
            logger.info("Model loaded successfully")
        except Exception as e:
            logger.error(f"Error loading model: {e}")
            # ëŒ€ì•ˆ ëª¨ë¸ë“¤ì„ ìˆœì„œëŒ€ë¡œ ì‹œë„
            alternative_models = [
                'paraphrase-multilingual-MiniLM-L12-v2',
                'all-MiniLM-L6-v2', 
                'distiluse-base-multilingual-cased'
            ]
            
            for alt_model in alternative_models:
                try:
                    logger.info(f"Trying alternative model: {alt_model}")
                    self.model = SentenceTransformer(alt_model)
                    self.model_name = alt_model
                    logger.info(f"Successfully loaded: {alt_model}")
                    break
                except Exception as e2:
                    logger.error(f"Error loading {alt_model}: {e2}")
                    continue
            
            if not self.model:
                logger.error("All model loading attempts failed")
    
    def process_all_documents(self) -> Dict[str, Any]:
        """ëª¨ë“  ë¬¸ì„œë¥¼ ì²˜ë¦¬í•˜ì—¬ ì„ë² ë”© ìƒì„±"""
        if not self.model:
            return {"success": False, "error": "Model not available"}
        
        logger.info("Starting document processing...")
        
        # ë¬¸ì„œ í”„ë¡œì„¸ì„œ ì´ˆê¸°í™”
        processor = DocumentProcessor(str(self.documents_path))
        
        # ëª¨ë“  ë¬¸ì„œ íŒŒì¼ ê°€ì ¸ì˜¤ê¸°
        document_files = processor.get_all_documents()
        
        if not document_files:
            return {"success": False, "error": "No documents found"}
        
        all_chunks = []
        chunk_metadata = []
        
        # ê° ë¬¸ì„œ ì²˜ë¦¬
        for doc_file in document_files:
            logger.info(f"Processing: {doc_file}")
            
            try:
                # ë¬¸ì„œë¥¼ ì²­í¬ë¡œ ë¶„í• 
                chunks = processor.process_document(doc_file)
                
                for chunk_data in chunks:
                    all_chunks.append(chunk_data['content'])
                    chunk_metadata.append(chunk_data['metadata'])
                    
            except Exception as e:
                logger.error(f"Error processing {doc_file}: {e}")
                continue
        
        if not all_chunks:
            return {"success": False, "error": "No valid chunks created"}
        
        logger.info(f"Total chunks created: {len(all_chunks)}")
        
        # ì„ë² ë”© ìƒì„±
        try:
            logger.info("Generating embeddings...")
            embeddings = self.model.encode(
                all_chunks,
                batch_size=32,
                show_progress_bar=True,
                convert_to_numpy=True
            )
            logger.info(f"Generated embeddings shape: {embeddings.shape}")
            
        except Exception as e:
            logger.error(f"Error generating embeddings: {e}")
            return {"success": False, "error": f"Embedding generation failed: {e}"}
        
        # ê²°ê³¼ ì €ì¥
        result = self._save_embeddings(embeddings, all_chunks, chunk_metadata)
        
        if result["success"]:
            # ë©”íƒ€ë°ì´í„° ì—…ë°ì´íŠ¸
            self._update_metadata(len(all_chunks), len(document_files))
        
        return result
    
    def _save_embeddings(
        self, 
        embeddings: np.ndarray, 
        chunks: List[str], 
        metadata: List[Dict]
    ) -> Dict[str, Any]:
        """ì„ë² ë”©ê³¼ ê´€ë ¨ ë°ì´í„° ì €ì¥"""
        try:
            # í˜„ì¬ ì‹œê°„ ìŠ¤íƒ¬í”„
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            
            # ì„ë² ë”© ì €ì¥ (numpy í˜•ì‹)
            embeddings_file = self.embeddings_path / f"embeddings_{timestamp}.npy"
            np.save(embeddings_file, embeddings)
            
            # ì²­í¬ í…ìŠ¤íŠ¸ ì €ì¥
            chunks_file = self.chunks_path / f"chunks_{timestamp}.json"
            with open(chunks_file, 'w', encoding='utf-8') as f:
                json.dump(chunks, f, ensure_ascii=False, indent=2)
            
            # ë©”íƒ€ë°ì´í„° ì €ì¥
            metadata_file = self.chunks_path / f"metadata_{timestamp}.json"
            with open(metadata_file, 'w', encoding='utf-8') as f:
                json.dump(metadata, f, ensure_ascii=False, indent=2)
            
            # FAISS ì¸ë±ìŠ¤ ìƒì„± ë° ì €ì¥
            if FAISS_AVAILABLE:
                try:
                    index = faiss.IndexFlatIP(embeddings.shape[1])  # ì½”ì‚¬ì¸ ìœ ì‚¬ë„ìš©
                    
                    # L2 ì •ê·œí™” (ì½”ì‚¬ì¸ ìœ ì‚¬ë„ë¥¼ ìœ„í•´)
                    embeddings_normalized = embeddings / np.linalg.norm(embeddings, axis=1, keepdims=True)
                    index.add(embeddings_normalized.astype('float32'))
                    
                    index_file = self.embeddings_path / f"faiss_index_{timestamp}.index"
                    faiss.write_index(index, str(index_file))
                    
                    logger.info(f"FAISS index saved: {index_file}")
                except Exception as e:
                    logger.warning(f"FAISS index creation failed: {e}")
            
            # ìµœì‹  íŒŒì¼ ì •ë³´ ì €ì¥
            latest_info = {
                "timestamp": timestamp,
                "embeddings_file": str(embeddings_file),
                "chunks_file": str(chunks_file),
                "metadata_file": str(metadata_file),
                "embedding_shape": embeddings.shape,
                "model_name": self.model_name,
                "total_chunks": len(chunks)
            }
            
            latest_file = self.processed_path / "latest.json"
            with open(latest_file, 'w', encoding='utf-8') as f:
                json.dump(latest_info, f, ensure_ascii=False, indent=2)
            
            logger.info("Embeddings saved successfully")
            return {
                "success": True,
                "timestamp": timestamp,
                "total_chunks": len(chunks),
                "embedding_dimension": embeddings.shape[1],
                "files": latest_info
            }
            
        except Exception as e:
            logger.error(f"Error saving embeddings: {e}")
            return {"success": False, "error": f"Save failed: {e}"}
    
    def load_latest_embeddings(self) -> bool:
        """ìµœì‹  ì„ë² ë”© ë°ì´í„° ë¡œë“œ"""
        try:
            latest_file = self.processed_path / "latest.json"
            if not latest_file.exists():
                logger.warning("No latest embeddings found")
                return False
            
            with open(latest_file, 'r', encoding='utf-8') as f:
                latest_info = json.load(f)
            
            # ì„ë² ë”© ë¡œë“œ
            embeddings = np.load(latest_info["embeddings_file"])
            
            # ì²­í¬ í…ìŠ¤íŠ¸ ë¡œë“œ
            with open(latest_info["chunks_file"], 'r', encoding='utf-8') as f:
                chunks = json.load(f)
            
            # ë©”íƒ€ë°ì´í„° ë¡œë“œ
            with open(latest_info["metadata_file"], 'r', encoding='utf-8') as f:
                metadata = json.load(f)
            
            # FAISS ì¸ë±ìŠ¤ ë¡œë“œ
            if FAISS_AVAILABLE:
                index_pattern = f"faiss_index_{latest_info['timestamp']}.index"
                index_files = list(self.embeddings_path.glob(index_pattern))
                if index_files:
                    self.index = faiss.read_index(str(index_files[0]))
            
            # í´ë˜ìŠ¤ ë³€ìˆ˜ì— ì €ì¥
            self.embeddings = embeddings
            self.documents = chunks
            self.document_metadata = metadata
            
            logger.info(f"Loaded embeddings: {embeddings.shape}")
            return True
            
        except Exception as e:
            logger.error(f"Error loading embeddings: {e}")
            return False
    
    def search_similar(
        self, 
        query: str, 
        top_k: int = 5,
        threshold: float = 0.3
    ) -> List[Dict[str, Any]]:
        """ìœ ì‚¬í•œ ë¬¸ì„œ ì²­í¬ ê²€ìƒ‰"""
        if not hasattr(self, 'embeddings') or not hasattr(self, 'documents'):
            if not self.load_latest_embeddings():
                logger.error("No embeddings available for search")
                return []
        
        # ëª¨ë¸ì´ ì—†ìœ¼ë©´ ì§€ê¸ˆ ë¡œë“œ
        if not self.model:
            self._load_model()
            if not self.model:
                logger.error("Model loading failed during search")
                return []
        
        try:
            # ì¿¼ë¦¬ ì„ë² ë”© ìƒì„±
            query_embedding = self.model.encode([query], convert_to_numpy=True)
            query_normalized = query_embedding / np.linalg.norm(query_embedding)
            
            if FAISS_AVAILABLE and self.index:
                # FAISSë¥¼ ì‚¬ìš©í•œ ë¹ ë¥¸ ê²€ìƒ‰
                scores, indices = self.index.search(query_normalized.astype('float32'), top_k)
                
                results = []
                for i, (score, idx) in enumerate(zip(scores[0], indices[0])):
                    if score >= threshold:  # ì„ê³„ê°’ ì´ìƒë§Œ ë°˜í™˜
                        results.append({
                            "content": self.documents[idx],
                            "metadata": self.document_metadata[idx],
                            "similarity_score": float(score),
                            "rank": i + 1
                        })
                
            else:
                # numpyë¥¼ ì‚¬ìš©í•œ ì§ì ‘ ê³„ì‚°
                embeddings_normalized = self.embeddings / np.linalg.norm(self.embeddings, axis=1, keepdims=True)
                similarities = np.dot(embeddings_normalized, query_normalized.T).flatten()
                
                # ìƒìœ„ kê°œ ê²°ê³¼ ì„ íƒ
                top_indices = np.argsort(similarities)[::-1][:top_k]
                
                results = []
                for i, idx in enumerate(top_indices):
                    score = similarities[idx]
                    if score >= threshold:
                        results.append({
                            "content": self.documents[idx],
                            "metadata": self.document_metadata[idx],
                            "similarity_score": float(score),
                            "rank": i + 1
                        })
            
            logger.info(f"Search query: '{query}' - Found {len(results)} results")
            return results
            
        except Exception as e:
            logger.error(f"Error during search: {e}")
            return []
    
    def _update_metadata(self, total_chunks: int, total_documents: int):
        """ë©”íƒ€ë°ì´í„° íŒŒì¼ ì—…ë°ì´íŠ¸"""
        try:
            # ê¸°ì¡´ ë©”íƒ€ë°ì´í„° ë¡œë“œ
            if self.metadata_path.exists():
                with open(self.metadata_path, 'r', encoding='utf-8') as f:
                    metadata = json.load(f)
            else:
                metadata = {
                    "version": "1.0.0",
                    "categories": {},
                    "processing_status": {},
                    "embedding_config": {}
                }
            
            # ì—…ë°ì´íŠ¸
            metadata.update({
                "last_updated": datetime.now().isoformat(),
                "total_documents": total_documents,
                "processing_status": {
                    "raw_documents": total_documents,
                    "processed_documents": total_documents,
                    "total_chunks": total_chunks,
                    "embeddings_generated": total_chunks,
                    "indexed_documents": total_chunks
                },
                "embedding_config": {
                    "model": self.model_name,
                    "chunk_size": 512,
                    "chunk_overlap": 50,
                    "vector_dimension": getattr(self, 'embeddings', np.array([[]])).shape[-1] if hasattr(self, 'embeddings') else 384
                }
            })
            
            # ì €ì¥
            with open(self.metadata_path, 'w', encoding='utf-8') as f:
                json.dump(metadata, f, ensure_ascii=False, indent=2)
                
        except Exception as e:
            logger.error(f"Error updating metadata: {e}")


def main():
    """ì„ë² ë”© ìƒì„± ì‹¤í–‰"""
    embedder = VectorEmbedder()
    
    print("Starting embedding generation...")
    result = embedder.process_all_documents()
    
    if result["success"]:
        print(f"âœ… Embeddings generated successfully!")
        print(f"ğŸ“Š Total chunks: {result['total_chunks']}")
        print(f"ğŸ“ Embedding dimension: {result['embedding_dimension']}")
        print(f"â° Timestamp: {result['timestamp']}")
        
        # í…ŒìŠ¤íŠ¸ ê²€ìƒ‰
        print("\nğŸ” Testing search functionality...")
        test_results = embedder.search_similar("ë³´ì¦ê¸ˆ ë°˜í™˜", top_k=3)
        
        for i, result in enumerate(test_results, 1):
            print(f"\n{i}. ìœ ì‚¬ë„: {result['similarity_score']:.3f}")
            print(f"   íŒŒì¼: {result['metadata']['file_name']}")
            print(f"   ë‚´ìš©: {result['content'][:100]}...")
    
    else:
        print(f"âŒ Error: {result['error']}")


if __name__ == "__main__":
    main()